{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 1.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch as t\n",
    "\n",
    "t.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. `tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表1-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3175e+03,  4.5674e-41,  2.0071e-34],\n",
       "        [ 0.0000e+00,  4.4842e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a  # 数值取决于内存空间的状态，print时候可能overflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()  # 把tensor转为list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel()  # b中元素总个数，2*3，等价于b.nelement()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.9833e-34, 0.0000e+00, 2.0377e-34],\n",
       "         [0.0000e+00, 8.9683e-44, 0.0000e+00]]),\n",
       " tensor([2., 3.]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "c, d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2217, -0.1433,  0.4928],\n",
       "        [-1.1504, -0.1515,  1.1666]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 0, 4])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5)  # 长度为5的随机排列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=t.int)  # 对角线为1, 不要求行列数一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor`是在0.4版本新增加的一个新版本的创建tensor方法，使用的方法，和参数几乎和`np.array`完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159)\n",
    "print(\"scalar: %s, shape of sclar: %s\" % (scalar, scalar.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: tensor([1, 2]), shape of vector: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = t.tensor([1, 2])\n",
    "print(\"vector: %s, shape of vector: %s\" % (vector, vector.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(1, 2)  # 注意和t.tensor([1, 2])的区别\n",
    "tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 1.2000],\n",
       "         [2.2000, 3.1000],\n",
       "         [4.9000, 5.2000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix, matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([[0.11111, 0.222222, 0.3333333]], dtype=t.float64, device=t.device(\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3)  # 当某一维为-1的时候，会自动计算它的大小\n",
    "b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1)  # 注意形状，在第1维（下标从0开始）上增加“１”\n",
    "# 等价于 b[:,None]\n",
    "b[:, None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2)  # -2表示倒数第二个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0)  # 压缩第0维的“１”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze()  # 把所有维度为“1”的压缩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b  # a修改，b作为view之后的，也会跟着修改\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                  0,                 100,                   2],\n",
       "        [                  3,                   4,                   5],\n",
       "        [3761681309000480301, 3270801228861093430, 4050202140167714148]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3)  # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7509, -0.2519, -1.0005,  0.0845],\n",
       "        [ 0.7921,  1.2696, -0.4029,  0.0465],\n",
       "        [-0.4869,  0.8106,  0.3142, -1.5637]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7509, -0.2519, -1.0005,  0.0845])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]  # 第0行(下标从0开始)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7509,  0.7921, -0.4869])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0]  # 第0列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0005)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2]  # 第0行第2个元素，等价于a[0, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0845)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1]  # 第0行最后一个元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7509, -0.2519, -1.0005,  0.0845],\n",
       "        [ 0.7921,  1.2696, -0.4029,  0.0465]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2]  # 前两行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7509, -0.2519],\n",
       "        [ 0.7921,  1.2696]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2]  # 前两行，第0,1列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7509, -0.2519]])\n",
      "tensor([ 0.7509, -0.2519])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2])  # 第0行，前两列\n",
    "print(a[0, :2])  # 注意两者的区别：形状不同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None类似于np.newaxis, 为a新增了一个轴\n",
    "# 等价于a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None].shape  # 等价于a[None,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None, :].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4, 1, 1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None, :, None, None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7509, -0.2519, -1.0005,  0.0845],\n",
       "        [ 0.7921,  1.2696, -0.4029,  0.0465],\n",
       "        [-0.4869,  0.8106,  0.3142, -1.5637]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  # 查看a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False,  True, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1  # 返回一个ByteTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2696])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > 1]  # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间\n",
    "\n",
    "a > 1  # 获取位置\n",
    "a[a > 1]  # 获取位置中的元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  在PyTorch中，torch.LongTensor是一种张量数据结构，它用来存储整数类型的数据，具体是64位的整数（即long类型）。这种张量通常用于存储索引、整数标签或其他需要整数表示的数据。\n",
    "t.LongTensor([0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7509, -0.2519, -1.0005,  0.0845],\n",
       "        [ 0.7921,  1.2696, -0.4029,  0.0465]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0, 1])]  # 第0行和第1行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的选择函数如表1-2所示。\n",
    "\n",
    "表1-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0， 行\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1， 列\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "a.gather(0, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3, 2, 1, 0]]).t()\n",
    "a.gather(1, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "a.gather(0, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  3.],\n",
       "        [ 0.,  5.,  6.,  0.],\n",
       "        [ 0.,  9., 10.,  0.],\n",
       "        [12.,  0.,  0., 15.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4, 4)\n",
    "c.scatter_(1, index, b.float())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0]  # 依旧是tensor）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0].item()  # python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[0:1, 0:1, None]\n",
    "print(d.shape)\n",
    "d.item()  # 只包含一个元素的tensor即可调用tensor.item,与形状无关\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0].item()  ->\n",
    "# raise ValueError: only one element tensors can be converted to Python scalars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor共享内存。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0, 27).view(3, 3, 3)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]]  # x[1,1,2]和x[2,2,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]]  # x[2,0,1],x[1,0,1],x[0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...]  # x[0] 和 x[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n",
    "\n",
    " \n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lebin/anaconda3/envs/torch/lib/python3.8/site-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type(\"torch.DoubleTensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "a.dtype  # 现在a是DoubleTensor,dtype是float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type(\"torch.FloatTensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float()\n",
    "b.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.9166e-310, 6.9166e-310, 1.5810e-322],\n",
       "        [3.9525e-322, 6.2404e-316, 1.5020e-321]], dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2, 3)  # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a)  # 等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16)  # 可以修改某些属性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6735, 0.4093, 0.0782],\n",
       "        [0.1207, 0.7667, 0.6290]], dtype=torch.float64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4, 5, dtype=t.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3, 4])  #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表1-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3  # 等价于t.fmod(a, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a**2  # 等价于t.pow(a, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568, -0.9589]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_()  # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "print(b)\n",
    "b.sum(dim=0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b)\n",
    "b.sum(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  3],\n",
       "        [ 3,  7, 12]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1)  # 沿着行累加\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表1-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > b]  # a中大于b的元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([15.,  6.]), tensor([0, 0]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, i = t.max(b, dim=1)\n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素\n",
    "v, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表1-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  3.,  6.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[ 0.,  9.],\n",
      "        [ 3., 12.],\n",
      "        [ 6., 15.]])\n"
     ]
    }
   ],
   "source": [
    "b.contiguous()\n",
    "\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones([2, 3], dtype=np.float32)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)  # 也可以直接将numpy对象传入Tensor, 注意是写Tensor\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()  # a, b, c三个对象共享内存\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "# 注意和上面的a的区别（dtype不是float32）\n",
    "a.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)  # 此处进行拷贝，不共享内存\n",
    "b.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a)  # 注意c的类型（DoubleTensor）\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b  # b与a不共享内存，所以即使a改变了，b也不变\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c  # c与a共享内存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.tensor(a)  # 注意是小写tensor\n",
    "\n",
    "tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0., 100.,   1.],\n",
      "        [  1.,   1.,   1.]], dtype=torch.float64)\n",
      "[[  1. 100.   1.]\n",
      " [  1.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "tensor[0, 0] = 0\n",
    "print(tensor)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.]]]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3, 1)\n",
    "a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2, 3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 100\n",
       " -100\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:]\n",
    "c.storage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127245456, 127245440)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr()  # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  100, -100,    3,    4,    5])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100  # c[0]的内存地址对应a[2]的内存地址\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6666,  100, -100],\n",
       "        [   3,    4,    5]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.LongTensor(c.storage())\n",
    "d[0] = 6666\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 0)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2]  # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6762, -0.2641, -0.1957, -0.2886],\n",
       "        [-0.0182,  1.5229,  1.3009, -0.9262],\n",
       "        [-0.0893, -0.0587, -1.2117, -0.1122]], device='cuda:0')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3, 4, device=t.device(\"cuda:0\"))\n",
    "    # 等价于\n",
    "    # a.t.randn(3,4).cuda(1)\n",
    "    # 但是前者更快\n",
    "    a.device\n",
    "\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6762, -0.2641, -0.1957, -0.2886],\n",
       "        [-0.0182,  1.5229,  1.3009, -0.9262],\n",
       "        [-0.0893, -0.0587, -1.2117, -0.1122]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cpu\")\n",
    "a.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(0)  # 把a转为GPU1上的tensor,\n",
    "    t.save(a, \"a.pth\")\n",
    "\n",
    "    # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)\n",
    "    b = t.load(\"a.pth\")\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load(\"a.pth\", map_location=lambda storage, loc: storage)\n",
    "    # 加载为d, 存储于GPU0上\n",
    "    d = t.load(\"a.pth\", map_location={\"cuda:1\": \"cuda:0\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i, j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12 ms ± 69.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 7.16 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "12.7 µs ± 14.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19999999) tensor(19999998)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(19999999), tensor(19999998))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000)\n",
    "print(a[-1], a[-2])  # 32bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b)  # 64bit的LongTensor不会溢出\n",
    "b[-1], b[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0170, -0.1281, -3.3774],\n",
       "        [-0.3852,  0.5982, -0.4918]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2, 3)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0170045551, -0.1280542612, -3.3773689270],\n",
       "        [-0.3852284253,  0.5981656313, -0.4917858541]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device(\"cuda:0\")  # 如果你想用gpu，改成t.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000)\n",
    "\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    \"\"\"产生随机数据：y=x*2+3，加上了一些噪声\"\"\"\n",
    "    x = t.rand(batch_size, 1, device=device) * 5\n",
    "    y = x * 2 + 3 + t.randn(batch_size, 1, device=device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f51eb6c7100>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=16)\n",
    "plt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  2.1024889945983887 b:  2.9769670963287354\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1).to(device)\n",
    "b = t.zeros(1, 1).to(device)\n",
    "\n",
    "lr = 0.02  # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4)\n",
    "\n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y)  # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2  # 均方误差\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "\n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "\n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "\n",
    "    if ii % 50 == 0:\n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6).view(-1, 1).to(device)\n",
    "        y = x.float().mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.cpu().numpy(), y.cpu().numpy())  # predicted\n",
    "\n",
    "        x2, y2 = get_fake_data(batch_size=32)\n",
    "        plt.scatter(x2.cpu().numpy(), y2.cpu().numpy())  # true data\n",
    "\n",
    "        plt.xlim(0, 5)\n",
    "        plt.ylim(0, 13)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "\n",
    "print(\"w: \", w.item(), \"b: \", b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
